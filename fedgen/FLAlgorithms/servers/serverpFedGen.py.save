from fedgen.FLAlgorithms.users.userpFedGen import UserpFedGen
from .serverbase import Server
import sys
from utils.model_utils import read_data, read_user_data, aggregate_user_data, create_generative_model, sample_multi_label_distribution
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torchvision.utils import save_image
import os
import copy
import time
from torch.utils.data import ConcatDataset
MIN_SAMPLES_PER_LABEL=1
from torchmetrics import Accuracy
import wandb

class FedGen(Server):
    def __init__(self, config, data, model, seed):
        super().__init__(config, data, model, seed)

        # Initialize data for all users
        #data = read_data(config['dataset)
        # data contains: clients, groups, train_data, test_data, proxy_data
        clients = data[0]
        total_users = copy.copy(clients)
        total_users.remove(config['test_country'])
        self.total_test_samples = 0
        self.local = 'local' in self.algorithm.lower()
        self.use_adam = 'adam' in self.algorithm.lower()

        self.early_stop = 20  # stop using generated samples after 20 local epochs
        self.student_model = copy.deepcopy(self.model)
        self.generative_model = create_generative_model(config['dataset'], config['algorithm'], self.model_name, config['embedding'])

        if not config['train']:
            print('number of generator parameteres: [{}]'.format(self.generative_model.get_number_of_parameters()))
            print('nu [{}]'.format(self.model.get_number_of_parameters()))
        self.latent_layer_idx = self.generative_model.latent_layer_idx
        self.init_ensemble_configs()
        print("latent_layer_idx: {}".format(self.latent_layer_idx))
        print("label embedding {}".format(self.generative_model.embedding))
        print("ensemeble learning rate: {}".format(self.ensemble_lr))
        print("ensemeble alpha = {}, beta = {}, eta = {}".format(self.ensemble_alpha, self.ensemble_beta, self.ensemble_eta))
        print("generator alpha = {}, beta = {}".format(self.generative_alpha, self.generative_beta))
        self.init_loss_fn()
        """
        The datasets seems to be combined here however one is not supposed to do this in federated learning apporaches. Not sure about the 
        point of this
        self.combined_data = ConcatDataset(list(data[2].values()))
        self.train_iter = iter(self.train_data_loader)
        train_data, test_data, train_loaders, val_loaders = data[2], data[3], data[4], data[5]
        #self.train_data_loader, self.train_iter, self.available_labels = aggregate_user_data(data, config['dataset'], self.ensemble_batch_size)
        """
        self.available_labels = {
        "Urban fabric": 0,
        "Industrial or commercial units": 1,
        "Arable land": 2,
        "Permanent crops": 3,
        "Pastures": 4,
        "Complex cultivation patterns": 5,
        "Land principally occupied by agriculture, with significant areas of natural vegetation": 6,
        "Agro-forestry areas": 7,
        "Broad-leaved forest": 8,
        "Coniferous forest": 9,
        "Mixed forest": 10,
        "Natural grassland and sparsely vegetated areas": 11,
        "Moors, heathland and sclerophyllous vegetation": 12,
        "Transitional woodland, shrub": 13,
        "Beaches, dunes, sands": 14,
        "Inland wetlands": 15,
        "Coastal wetlands": 16,
        "Inland waters": 17,
        "Marine waters": 18
        }
        self.gen_batch_size = config['gen_batch_size']
        self.generative_optimizer = torch.optim.AdamW(
            params=self.generative_model.parameters(),
            lr=self.ensemble_lr, betas=(0.9, 0.999),
            eps=1e-08, weight_decay=self.weight_decay, amsgrad=False)
        self.generative_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(
            optimizer=self.generative_optimizer, gamma=0.98)
        self.optimizer = torch.optim.AdamW(
            params=self.model.parameters(),
            lr=self.ensemble_lr, betas=(0.9, 0.999),
            eps=1e-08, weight_decay=0, amsgrad=False)
        self.lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=self.optimizer, gamma=0.98)

        #### creating users ####
        self.users = []
        test_data = data[3]
        self.total_test_samples = len(test_data)
        for id in total_users:
            train_data = data[2][id]
            #the following code used to obtain label frequency in the dataset and dataloaders which we already have
            #label_info = read_user_data(id, data, config['dataset'], count_labels=True)
            self.total_train_samples+=len(train_data)
            user=UserpFedGen(
                config, id, model, self.generative_model,
                train_data, test_data,
                self.available_labels, self.latent_layer_idx, config['label_frequencies'][data[6][id]], #data[6] contains id to country mapping
                use_adam=self.use_adam)
            self.users.append(user)
        print("Number of Train/Test samples:", self.total_train_samples, self.total_test_samples)
        print("Data from {} users in total.".format(total_users))
        print("Finished creating FedAvg server.")

    def train(self):
        #### pretraining
        for glob_iter in range(self.num_glob_iters):
            print("\n\n-------------Round number: ",glob_iter, " -------------\n\n")
            self.selected_users, self.user_idxs=self.select_users(glob_iter, self.num_users, return_idx=True)
            #what is the difference between self.local being True or False
            if not self.local:
                self.send_parameters(mode=self.mode) # broadcast averaged prediction model
            #self.evaluate()
            chosen_verbose_user = np.random.randint(0, len(self.users))
            self.timestamp = time.time() # log user-training start time
            while(True):
                for user_id, user in zip(self.user_idxs, self.selected_users): # allow selected users to train
                    verbose= user_id == chosen_verbose_user
                    # perform regularization using generated samples after the first communication round
                    user.train(
                        glob_iter,
                        personalized=self.personalized,
                        early_stop=self.early_stop,
                        verbose=verbose and glob_iter > 0,
                        regularization= glob_iter > 0)
                  

            curr_timestamp = time.time() # log  user-training end time
            train_time = (curr_timestamp - self.timestamp) / len(self.selected_users)
            self.metrics['user_train_time'].append(train_time)

            #if self.personalized:
            #    self.evaluate_personalized_model()
            self.timestamp = time.time() # log server-agg start time
            self.train_generator(
                self.batch_size,
                epoches=self.ensemble_epochs // self.n_teacher_iters,
                latent_layer_idx=self.latent_layer_idx,
                verbose=True
            )
            self.aggregate_parameters()
            curr_timestamp=time.time()  # log  server-agg end time
            agg_time = curr_timestamp - self.timestamp
            self.metrics['server_agg_time'].append(agg_time)
            #if glob_iter  > 0 and glob_iter % 20 == 0 and self.latent_layer_idx == 0:
            #    self.visualize_images(self.generative_model, glob_iter, repeats=10)

        #self.save_results(args)
        #self.save_model()

    def evaluate(self, save=True, selected=False):
        # override evaluate function to log vae-loss.
        test_ids, test_samples, test_accs, test_losses = self.test(selected=selected)
        glob_acc = np.sum(test_accs) * 1.0 / np.sum(test_samples)
        glob_loss = np.sum([x * y for (x, y) in zip(test_samples, test_losses)]).item() / np.sum(test_samples)
        if save:
            self.metrics['glob_acc'].append(glob_acc)
            self.metrics['glob_loss'].append(glob_loss)
        print("Average Global Accurancy = {:.4f}, Loss = {:.2f}.".format(glob_acc, glob_loss))

    def test(self, selected=False):
        '''tests self.latest_model on given clients
        '''
        num_samples = []
        tot_correct = []
        losses = []
        user_accuracies = {}
        users = self.selected_users if selected else self.users
        for c in users:
            acc, c_loss, ns = c.test()
            #tot_correct.append(ct * 1.0)
            #num_samples.append(ns)
            losses.append(c_loss)
            user_accuracies[c.id] = acc
        ids = [c.id for c in self.users]
        return ids, user_accuracies, losses

    def train_generator(self, batch_size, epoches=1, latent_layer_idx=-1, verbose=False):
        """
        Learn a generator that find a consensus latent representation z, given a label 'y'.
        :param batch_size:
        :param epoches:
        :param latent_layer_idx: if set to -1 (-2), get latent representation of the last (or 2nd to last) layer.
        :param verbose: print loss information.
        :return: Do not return anything.
        """
        #self.generative_regularizer.train()
        self.label_weights, self.qualified_labels = self.get_label_weights()
        TEACHER_LOSS, STUDENT_LOSS, DIVERSITY_LOSS, STUDENT_LOSS2 = 0, 0, 0, 0

        def update_generator_(n_iters, student_model, TEACHER_LOSS, STUDENT_LOSS, DIVERSITY_LOSS):
            self.generative_model.train()
            student_model.eval()
            for i in range(n_iters):
                self.generative_optimizer.zero_grad()
                y_sampled = sample_multi_label_distribution(len(self.available_labels), self.gen_batch_size)
                #which labels have been selected for every sampled y
                labels_in_sample = [np.where(y_i == 1)[0] for y_i in y_sampled]
                print('labels in sample: ', labels_in_sample)
                #y=np.random.choice(self.qualified_labels, batch_size)
                y_sampled = torch.from_numpy(y_sampled).float()
                ## feed to generator
                gen_result=self.generative_model(y_sampled, latent_layer_idx=latent_layer_idx, verbose=True)
                # get approximation of Z( latent) if latent set to True, X( raw image) otherwise
                gen_output, eps=gen_result['output'], gen_result['eps']
                ##### get losses ####
                # decoded = self.generative_regularizer(gen_output)
                # regularization_loss = beta * self.generative_model.dist_loss(decoded, eps) # map generated z back to eps
                diversity_loss=self.generative_model.diversity_loss(eps, gen_output)  # encourage different outputs
                ######### get teacher loss ############
                teacher_loss=0
                teacher_logit=0
                for user_idx, user in enumerate(self.selected_users):
                    user.model.eval()
                    #every labels' weights in the user's data
                    label_weights=self.label_weights[:, user_idx].reshape(-1, 1)
                    #sample's label weights is averaged accordingly
                    weight = [[label_weights[i] for i in ls] for ls in labels_in_sample]
                    print('weights of labels ', weight)
                    weight = torch.tensor([sum(w)/len(w) for w in weight])      
                    expand_weight=np.tile(weight, (1, self.unique_labels))
                    print('expand weight shape: ', expand_weight.shape)
                    user_result_given_gen=user.model(gen_output, start_layer_idx=latent_layer_idx, logit=True)
                    teacher_loss_=torch.mean( \
                        self.loss(user_result_given_gen['output'], y_sampled) * \
                        torch.tensor(weight, dtype=torch.float32))
                    teacher_loss+=teacher_loss_
                    teacher_logit+=user_result_given_gen['logit'] * torch.tensor(expand_weight, dtype=torch.float32)

                ######### get student loss ############
                student_output=student_model(gen_output, start_layer_idx=latent_layer_idx, logit=True)
                student_loss=F.kl_div(F.log_softmax(student_output['logit'], dim=1), F.softmax(teacher_logit, dim=1))
                if self.ensemble_beta > 0:
                    loss=self.ensemble_alpha * teacher_loss - self.ensemble_beta * student_loss + self.ensemble_eta * diversity_loss
                else:
                    loss=self.ensemble_alpha * teacher_loss + self.ensemble_eta * diversity_loss
                loss.backward()
                self.generative_optimizer.step()
                TEACHER_LOSS += self.ensemble_alpha * teacher_loss#(torch.mean(TEACHER_LOSS.double())).item()
                STUDENT_LOSS += self.ensemble_beta * student_loss#(torch.mean(student_loss.double())).item()
                DIVERSITY_LOSS += self.ensemble_eta * diversity_loss#(torch.mean(diversity_loss.double())).item()
                wandb.log({'total_teacher_loss' : TEACHER_LOSS, 
                        'total_student_loss' : STUDENT_LOSS, 
                        'total_diversity_loss' : DIVERSITY_LOSS, 
                })

            return TEACHER_LOSS, STUDENT_LOSS, DIVERSITY_LOSS

        for i in range(epoches):
            TEACHER_LOSS, STUDENT_LOSS, DIVERSITY_LOSS=update_generator_(
                self.n_teacher_iters, self.model, TEACHER_LOSS, STUDENT_LOSS, DIVERSITY_LOSS)

        TEACHER_LOSS = TEACHER_LOSS.detach().numpy() / (self.n_teacher_iters * epoches)
        STUDENT_LOSS = STUDENT_LOSS.detach().numpy() / (self.n_teacher_iters * epoches)
        DIVERSITY_LOSS = DIVERSITY_LOSS.detach().numpy() / (self.n_teacher_iters * epoches)
        info="Generator: Teacher Loss= {:.4f}, Student Loss= {:.4f}, Diversity Loss = {:.4f}, ". \
            format(TEACHER_LOSS, STUDENT_LOSS, DIVERSITY_LOSS)
        if verbose:
            print(info)
        self.generative_lr_scheduler.step()

    def get_label_weights(self):
        label_weights = []
        qualified_labels = []
        print(self.unique_labels, ' are the number of unique labels')
        for label in range(self.unique_labels):
            weights = []
            for user in self.selected_users:
                weights.append(user.label_counts[label])
            if np.max(weights) > MIN_SAMPLES_PER_LABEL:
                qualified_labels.append(label)
            # uniform
            label_weights.append( np.array(weights) / np.sum(weights) )
        label_weights = np.array(label_weights).reshape((self.unique_labels, -1))
        #label weight is the weight of every label in every user
        return label_weights, qualified_labels




